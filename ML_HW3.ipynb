{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNnrjkIUWEDh"
   },
   "source": [
    "# Homework 3\n",
    "\n",
    "DUE Nov 15th at 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbpTqmT4WhaB"
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "In this problem, you will implement a simple feed-forward neural network using PyTorch, a straight-forward and simple-to-pickup framework for quickly prototyping deep learning model. \n",
    "\n",
    "PyTorch provides 2 powerful things. First, a nice data structure called Tensor (basically a matrix, similar to Numpy ndarray). Tensor is optimized for matrix calculation and can be loaded to a GPU. Tensor is also implemented so that it's easy to calculate and pass back chains of gradients, which is extremely useful for backpropagation on neural network. Second, a nice inner mechanism called Autograd that nicely maps variables involved a chain of calculations and efficiently calculates their gradients via the chain rule when needed. Read more here: https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95  \n",
    "\n",
    "You will define a neural network class in PyTorch and use the network to learn a classification task on the famous KDD CUP 99 dataset. You can refer to Problem 2 to see how a network class can be defined, how to use a PyTorch's DataLoader, and how a training loop may looks like.\n",
    "\n",
    "There are many greate tutorial on PyTorch out there. For example, this video on Youtube explains how to build a simple network in PyTorch quite clearly: https://www.youtube.com/watch?v=oPhxf2fXHkQ\n",
    "\n",
    "### Part a\n",
    "Firstly, load and inspect the \"**KDD CUP 99**\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qyL0okEBWggC"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "import torch\n",
    "\n",
    "X, y = fetch_kddcup99(return_X_y=True, percent10=True)\n",
    "\n",
    "# enc = OrdinalEncoder()\n",
    "# enc.fit(X[:,1:4])\n",
    "# X[:,1:4] = enc.transform(X[:,1:4])\n",
    "\n",
    "# lab = LabelEncoder()\n",
    "# lab.fit(y)\n",
    "# y = lab.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, b'tcp', b'http', ..., 0.0, 0.0, 0.0],\n",
       "       [0, b'tcp', b'http', ..., 0.0, 0.0, 0.0],\n",
       "       [0, b'tcp', b'http', ..., 0.0, 0.0, 0.0],\n",
       "       ...,\n",
       "       [0, b'tcp', b'http', ..., 0.01, 0.0, 0.0],\n",
       "       [0, b'tcp', b'http', ..., 0.01, 0.0, 0.0],\n",
       "       [0, b'tcp', b'http', ..., 0.01, 0.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD09sAm-52_u"
   },
   "source": [
    "Split them into a train set (70%), a validation set (10%), and a test set (20%). Then, create a PyTorch's DataLoader for the train set, a DataLoader for the validation set, and a DataLoader for the test set.\n",
    "\n",
    "You can read about PyTorch's DataLoader from:\n",
    "\n",
    "*   https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "*   https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, X, y):\n",
    "        #'Initialization'\n",
    "        self.labels = torch.tensor(y)\n",
    "        self.features = torch.tensor(X)\n",
    "\n",
    "    def __len__(self):\n",
    "        #'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #'Generates one sample of data'\n",
    "        # Select sample\n",
    "        return self.features[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "tnwd2ylj5BD4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Define test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    test_size=0.2, random_state=123,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "# Split part of train into validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                 test_size = 0.125, random_state=123,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "X_train = np.array(X_train, dtype=float)\n",
    "X_val = np.array(X_val, dtype=float)\n",
    "X_test = np.array(X_test, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets to input to loaders\n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "val_dataset = Dataset(X_val, y_val)\n",
    "test_dataset = Dataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuEYUkVdWpYw"
   },
   "source": [
    "### Part b \n",
    "Create a Python class for our neural network model. The network should have 1 input layer, 1 hidden layer, and 1 output layer. You are free to choose the size of the hidden layer (it may affect the performance). Use ReLU as the activation function (torch.relu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 41\n",
    "hidden_size = 64\n",
    "num_classes = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "68ti8_83WzHP"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Any Pytorch's network class is an extension of the torch.nn.Module parent class.\n",
    "# To define a network class, you need to define at least 2 methods: \n",
    "# an __init__() method (constructor) and a forward() method\n",
    "class SimpleNetwork(torch.nn.Module):\n",
    "    # Create the network class by filling in this block of code\n",
    "\n",
    "    # Create the constructor. Add any additional arguments as you wish\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes) \n",
    "    \n",
    "    # Define the feed forward function.\n",
    "    # x is the input example/examples.\n",
    "    # Add any additional arguments as you wish.\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONBn93ldWz8n"
   },
   "source": [
    "### Part c \n",
    "Train the network using the training dataset. Use the SGD optimizer and CrossEntropyLoss. After each epoch, record the current loss and the current training accuracy. The current training accuracy is obtained by evaluating the model on the train dataset. Use the DataLoaders defined in part a to efficiently pass training and testing data.\n",
    "\n",
    "You can learn about the available optimizers at:\n",
    "https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "You can learn about the available loss functions at:\n",
    "https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 112.0067\n",
      "Accuracy of the network on training set: 64.9254 %\n",
      "Accuracy of the network on validation set: 65.06 %\n",
      "Epoch [2/100], Loss: 0.5963\n",
      "Accuracy of the network on training set: 80.2319 %\n",
      "Accuracy of the network on validation set: 79.94 %\n",
      "Epoch [3/100], Loss: 0.4339\n",
      "Accuracy of the network on training set: 81.0783 %\n",
      "Accuracy of the network on validation set: 80.8 %\n",
      "Epoch [4/100], Loss: 0.6055\n",
      "Accuracy of the network on training set: 80.1246 %\n",
      "Accuracy of the network on validation set: 79.87 %\n",
      "Epoch [5/100], Loss: 0.4929\n",
      "Accuracy of the network on training set: 80.6246 %\n",
      "Accuracy of the network on validation set: 80.33 %\n",
      "Epoch [6/100], Loss: 0.4164\n",
      "Accuracy of the network on training set: 80.9198 %\n",
      "Accuracy of the network on validation set: 80.66 %\n",
      "Epoch [7/100], Loss: 0.4200\n",
      "Accuracy of the network on training set: 80.9103 %\n",
      "Accuracy of the network on validation set: 80.62 %\n",
      "Epoch [8/100], Loss: 0.3959\n",
      "Accuracy of the network on training set: 80.8518 %\n",
      "Accuracy of the network on validation set: 80.56 %\n",
      "Epoch [9/100], Loss: 0.6321\n",
      "Accuracy of the network on training set: 80.7986 %\n",
      "Accuracy of the network on validation set: 80.55 %\n",
      "Epoch [10/100], Loss: 0.4591\n",
      "Accuracy of the network on training set: 81.0919 %\n",
      "Accuracy of the network on validation set: 80.78 %\n",
      "Epoch [11/100], Loss: 0.4037\n",
      "Accuracy of the network on training set: 81.1202 %\n",
      "Accuracy of the network on validation set: 80.82 %\n",
      "Epoch [12/100], Loss: 0.5305\n",
      "Accuracy of the network on training set: 80.0899 %\n",
      "Accuracy of the network on validation set: 79.8 %\n",
      "Epoch [13/100], Loss: 0.5205\n",
      "Accuracy of the network on training set: 80.1379 %\n",
      "Accuracy of the network on validation set: 79.87 %\n",
      "Epoch [14/100], Loss: 0.4428\n",
      "Accuracy of the network on training set: 80.1503 %\n",
      "Accuracy of the network on validation set: 79.88 %\n",
      "Epoch [15/100], Loss: 0.5225\n",
      "Accuracy of the network on training set: 80.8741 %\n",
      "Accuracy of the network on validation set: 80.59 %\n",
      "Epoch [16/100], Loss: 0.4725\n",
      "Accuracy of the network on training set: 80.9643 %\n",
      "Accuracy of the network on validation set: 80.69 %\n",
      "Epoch [17/100], Loss: 0.3620\n",
      "Accuracy of the network on training set: 81.0199 %\n",
      "Accuracy of the network on validation set: 80.75 %\n",
      "Epoch [18/100], Loss: 0.4256\n",
      "Accuracy of the network on training set: 80.8741 %\n",
      "Accuracy of the network on validation set: 80.6 %\n",
      "Epoch [19/100], Loss: 0.5597\n",
      "Accuracy of the network on training set: 80.9105 %\n",
      "Accuracy of the network on validation set: 80.65 %\n",
      "Epoch [20/100], Loss: 0.4471\n",
      "Accuracy of the network on training set: 81.0251 %\n",
      "Accuracy of the network on validation set: 80.76 %\n",
      "Epoch [21/100], Loss: 0.5164\n",
      "Accuracy of the network on training set: 81.0285 %\n",
      "Accuracy of the network on validation set: 80.75 %\n",
      "Epoch [22/100], Loss: 0.5334\n",
      "Accuracy of the network on training set: 81.0421 %\n",
      "Accuracy of the network on validation set: 80.77 %\n",
      "Epoch [23/100], Loss: 0.3617\n",
      "Accuracy of the network on training set: 81.0919 %\n",
      "Accuracy of the network on validation set: 80.81 %\n",
      "Epoch [24/100], Loss: 0.5227\n",
      "Accuracy of the network on training set: 80.9507 %\n",
      "Accuracy of the network on validation set: 80.68 %\n",
      "Epoch [25/100], Loss: 0.4347\n",
      "Accuracy of the network on training set: 81.0465 %\n",
      "Accuracy of the network on validation set: 80.78 %\n",
      "Epoch [26/100], Loss: 0.5830\n",
      "Accuracy of the network on training set: 81.0459 %\n",
      "Accuracy of the network on validation set: 80.77 %\n",
      "Epoch [27/100], Loss: 0.5943\n",
      "Accuracy of the network on training set: 81.0548 %\n",
      "Accuracy of the network on validation set: 80.78 %\n",
      "Epoch [28/100], Loss: 0.4437\n",
      "Accuracy of the network on training set: 81.1824 %\n",
      "Accuracy of the network on validation set: 80.89 %\n",
      "Epoch [29/100], Loss: 0.5831\n",
      "Accuracy of the network on training set: 80.6754 %\n",
      "Accuracy of the network on validation set: 80.39 %\n",
      "Epoch [30/100], Loss: 0.4350\n",
      "Accuracy of the network on training set: 81.0233 %\n",
      "Accuracy of the network on validation set: 80.75 %\n",
      "Epoch [31/100], Loss: 0.4116\n",
      "Accuracy of the network on training set: 81.2318 %\n",
      "Accuracy of the network on validation set: 80.94 %\n",
      "Epoch [32/100], Loss: 0.4170\n",
      "Accuracy of the network on training set: 81.2425 %\n",
      "Accuracy of the network on validation set: 80.94 %\n",
      "Epoch [33/100], Loss: 0.4724\n",
      "Accuracy of the network on training set: 81.2711 %\n",
      "Accuracy of the network on validation set: 80.98 %\n",
      "Epoch [34/100], Loss: 0.4598\n",
      "Accuracy of the network on training set: 81.1832 %\n",
      "Accuracy of the network on validation set: 80.89 %\n",
      "Epoch [35/100], Loss: 0.1042\n",
      "Accuracy of the network on training set: 98.9477 %\n",
      "Accuracy of the network on validation set: 98.94 %\n",
      "Epoch [36/100], Loss: 0.6946\n",
      "Accuracy of the network on training set: 42.0853 %\n",
      "Accuracy of the network on validation set: 42.06 %\n",
      "Epoch [37/100], Loss: 0.5031\n",
      "Accuracy of the network on training set: 81.0933 %\n",
      "Accuracy of the network on validation set: 80.83 %\n",
      "Epoch [38/100], Loss: 0.4371\n",
      "Accuracy of the network on training set: 81.1847 %\n",
      "Accuracy of the network on validation set: 80.88 %\n",
      "Epoch [39/100], Loss: 0.3963\n",
      "Accuracy of the network on training set: 81.1827 %\n",
      "Accuracy of the network on validation set: 80.88 %\n",
      "Epoch [40/100], Loss: 0.4179\n",
      "Accuracy of the network on training set: 81.1913 %\n",
      "Accuracy of the network on validation set: 80.9 %\n",
      "Epoch [41/100], Loss: 0.5508\n",
      "Accuracy of the network on training set: 80.3487 %\n",
      "Accuracy of the network on validation set: 80.07 %\n",
      "Epoch [42/100], Loss: 0.5251\n",
      "Accuracy of the network on training set: 81.0638 %\n",
      "Accuracy of the network on validation set: 80.77 %\n",
      "Epoch [43/100], Loss: 0.4144\n",
      "Accuracy of the network on training set: 81.0728 %\n",
      "Accuracy of the network on validation set: 80.79 %\n",
      "Epoch [44/100], Loss: 0.5570\n",
      "Accuracy of the network on training set: 81.0930 %\n",
      "Accuracy of the network on validation set: 80.82 %\n",
      "Epoch [45/100], Loss: 0.5640\n",
      "Accuracy of the network on training set: 81.0762 %\n",
      "Accuracy of the network on validation set: 80.79 %\n",
      "Epoch [46/100], Loss: 0.4187\n",
      "Accuracy of the network on training set: 81.0600 %\n",
      "Accuracy of the network on validation set: 80.78 %\n",
      "Epoch [47/100], Loss: 0.3815\n",
      "Accuracy of the network on training set: 81.1983 %\n",
      "Accuracy of the network on validation set: 80.92 %\n",
      "Epoch [48/100], Loss: 0.3395\n",
      "Accuracy of the network on training set: 81.0974 %\n",
      "Accuracy of the network on validation set: 80.82 %\n",
      "Epoch [49/100], Loss: 0.4505\n",
      "Accuracy of the network on training set: 81.0950 %\n",
      "Accuracy of the network on validation set: 80.79 %\n",
      "Epoch [50/100], Loss: 0.5068\n",
      "Accuracy of the network on training set: 81.2055 %\n",
      "Accuracy of the network on validation set: 80.9 %\n",
      "Epoch [51/100], Loss: 0.4581\n",
      "Accuracy of the network on training set: 81.1054 %\n",
      "Accuracy of the network on validation set: 80.82 %\n",
      "Epoch [52/100], Loss: 0.4862\n",
      "Accuracy of the network on training set: 81.1052 %\n",
      "Accuracy of the network on validation set: 80.82 %\n",
      "Epoch [53/100], Loss: 0.5072\n",
      "Accuracy of the network on training set: 81.1286 %\n",
      "Accuracy of the network on validation set: 80.85 %\n",
      "Epoch [54/100], Loss: 0.4483\n",
      "Accuracy of the network on training set: 81.2440 %\n",
      "Accuracy of the network on validation set: 80.94 %\n",
      "Epoch [55/100], Loss: 0.3213\n",
      "Accuracy of the network on training set: 83.2861 %\n",
      "Accuracy of the network on validation set: 83.02 %\n",
      "Epoch [56/100], Loss: 0.3901\n",
      "Accuracy of the network on training set: 83.6129 %\n",
      "Accuracy of the network on validation set: 83.34 %\n",
      "Epoch [57/100], Loss: 0.5030\n",
      "Accuracy of the network on training set: 83.4113 %\n",
      "Accuracy of the network on validation set: 83.14 %\n",
      "Epoch [58/100], Loss: 0.4304\n",
      "Accuracy of the network on training set: 80.5549 %\n",
      "Accuracy of the network on validation set: 80.26 %\n",
      "Epoch [59/100], Loss: 0.4358\n",
      "Accuracy of the network on training set: 83.2766 %\n",
      "Accuracy of the network on validation set: 83.06 %\n",
      "Epoch [60/100], Loss: 0.3693\n",
      "Accuracy of the network on training set: 83.3454 %\n",
      "Accuracy of the network on validation set: 83.11 %\n",
      "Epoch [61/100], Loss: 0.4241\n",
      "Accuracy of the network on training set: 83.4159 %\n",
      "Accuracy of the network on validation set: 83.18 %\n",
      "Epoch [62/100], Loss: 0.4106\n",
      "Accuracy of the network on training set: 83.7462 %\n",
      "Accuracy of the network on validation set: 83.49 %\n",
      "Epoch [63/100], Loss: 0.4020\n",
      "Accuracy of the network on training set: 80.3270 %\n",
      "Accuracy of the network on validation set: 80.1 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100], Loss: 0.4535\n",
      "Accuracy of the network on training set: 80.4875 %\n",
      "Accuracy of the network on validation set: 80.25 %\n",
      "Epoch [65/100], Loss: 0.5326\n",
      "Accuracy of the network on training set: 82.0201 %\n",
      "Accuracy of the network on validation set: 81.73 %\n",
      "Epoch [66/100], Loss: 0.4580\n",
      "Accuracy of the network on training set: 80.4936 %\n",
      "Accuracy of the network on validation set: 80.27 %\n",
      "Epoch [67/100], Loss: 0.6374\n",
      "Accuracy of the network on training set: 80.3845 %\n",
      "Accuracy of the network on validation set: 80.11 %\n",
      "Epoch [68/100], Loss: 0.3933\n",
      "Accuracy of the network on training set: 81.2176 %\n",
      "Accuracy of the network on validation set: 80.94 %\n",
      "Epoch [69/100], Loss: 0.5192\n",
      "Accuracy of the network on training set: 81.6228 %\n",
      "Accuracy of the network on validation set: 81.34 %\n",
      "Epoch [70/100], Loss: 0.5353\n",
      "Accuracy of the network on training set: 81.6265 %\n",
      "Accuracy of the network on validation set: 81.31 %\n",
      "Epoch [71/100], Loss: 0.4523\n",
      "Accuracy of the network on training set: 81.5832 %\n",
      "Accuracy of the network on validation set: 81.26 %\n",
      "Epoch [72/100], Loss: 0.4149\n",
      "Accuracy of the network on training set: 82.5114 %\n",
      "Accuracy of the network on validation set: 82.27 %\n",
      "Epoch [73/100], Loss: 0.4766\n",
      "Accuracy of the network on training set: 83.1965 %\n",
      "Accuracy of the network on validation set: 82.98 %\n",
      "Epoch [74/100], Loss: 0.5230\n",
      "Accuracy of the network on training set: 83.3168 %\n",
      "Accuracy of the network on validation set: 83.11 %\n",
      "Epoch [75/100], Loss: 0.4658\n",
      "Accuracy of the network on training set: 83.2005 %\n",
      "Accuracy of the network on validation set: 82.98 %\n",
      "Epoch [76/100], Loss: 0.4168\n",
      "Accuracy of the network on training set: 81.5381 %\n",
      "Accuracy of the network on validation set: 81.23 %\n",
      "Epoch [77/100], Loss: 0.4869\n",
      "Accuracy of the network on training set: 83.5076 %\n",
      "Accuracy of the network on validation set: 83.27 %\n",
      "Epoch [78/100], Loss: 0.5952\n",
      "Accuracy of the network on training set: 83.6580 %\n",
      "Accuracy of the network on validation set: 83.44 %\n",
      "Epoch [79/100], Loss: 0.4281\n",
      "Accuracy of the network on training set: 83.6837 %\n",
      "Accuracy of the network on validation set: 83.46 %\n",
      "Epoch [80/100], Loss: 0.4975\n",
      "Accuracy of the network on training set: 83.5652 %\n",
      "Accuracy of the network on validation set: 83.34 %\n",
      "Epoch [81/100], Loss: 0.4093\n",
      "Accuracy of the network on training set: 83.5594 %\n",
      "Accuracy of the network on validation set: 83.31 %\n",
      "Epoch [82/100], Loss: 0.4961\n",
      "Accuracy of the network on training set: 83.7222 %\n",
      "Accuracy of the network on validation set: 83.49 %\n",
      "Epoch [83/100], Loss: 0.3804\n",
      "Accuracy of the network on training set: 83.5897 %\n",
      "Accuracy of the network on validation set: 83.37 %\n",
      "Epoch [84/100], Loss: 0.5407\n",
      "Accuracy of the network on training set: 83.4547 %\n",
      "Accuracy of the network on validation set: 83.21 %\n",
      "Epoch [85/100], Loss: 0.5036\n",
      "Accuracy of the network on training set: 83.7022 %\n",
      "Accuracy of the network on validation set: 83.46 %\n",
      "Epoch [86/100], Loss: 0.4343\n",
      "Accuracy of the network on training set: 83.4504 %\n",
      "Accuracy of the network on validation set: 83.2 %\n",
      "Epoch [87/100], Loss: 0.4536\n",
      "Accuracy of the network on training set: 83.6372 %\n",
      "Accuracy of the network on validation set: 83.4 %\n",
      "Epoch [88/100], Loss: 0.3002\n",
      "Accuracy of the network on training set: 83.7453 %\n",
      "Accuracy of the network on validation set: 83.51 %\n",
      "Epoch [89/100], Loss: 0.5756\n",
      "Accuracy of the network on training set: 83.6068 %\n",
      "Accuracy of the network on validation set: 83.38 %\n",
      "Epoch [90/100], Loss: 0.4668\n",
      "Accuracy of the network on training set: 83.7502 %\n",
      "Accuracy of the network on validation set: 83.52 %\n",
      "Epoch [91/100], Loss: 0.5210\n",
      "Accuracy of the network on training set: 83.7210 %\n",
      "Accuracy of the network on validation set: 83.48 %\n",
      "Epoch [92/100], Loss: 0.4359\n",
      "Accuracy of the network on training set: 83.7442 %\n",
      "Accuracy of the network on validation set: 83.5 %\n",
      "Epoch [93/100], Loss: 0.4162\n",
      "Accuracy of the network on training set: 83.7476 %\n",
      "Accuracy of the network on validation set: 83.5 %\n",
      "Epoch [94/100], Loss: 0.4263\n",
      "Accuracy of the network on training set: 83.6583 %\n",
      "Accuracy of the network on validation set: 83.42 %\n",
      "Epoch [95/100], Loss: 0.3675\n",
      "Accuracy of the network on training set: 83.7615 %\n",
      "Accuracy of the network on validation set: 83.51 %\n",
      "Epoch [96/100], Loss: 0.4359\n",
      "Accuracy of the network on training set: 83.7887 %\n",
      "Accuracy of the network on validation set: 83.56 %\n",
      "Epoch [97/100], Loss: 0.4512\n",
      "Accuracy of the network on training set: 81.0644 %\n",
      "Accuracy of the network on validation set: 80.78 %\n",
      "Epoch [98/100], Loss: 0.5873\n",
      "Accuracy of the network on training set: 81.2150 %\n",
      "Accuracy of the network on validation set: 80.93 %\n",
      "Epoch [99/100], Loss: 0.5753\n",
      "Accuracy of the network on training set: 81.1517 %\n",
      "Accuracy of the network on validation set: 80.87 %\n",
      "Epoch [100/100], Loss: 0.5022\n",
      "Accuracy of the network on training set: 81.1749 %\n",
      "Accuracy of the network on validation set: 80.9 %\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleNetwork(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 100\n",
    "\n",
    "# number of parameters in model is equal to number of hidden channels\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)  \n",
    "\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "losses = []\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(features.float())  \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Reset the gradients, backpropogate, optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  # backwards pass\n",
    "        optimizer.step()  # gradient descent\n",
    "        \n",
    "        if i == 0:\n",
    "            losses.append(loss.item())\n",
    "            print (f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}')\n",
    "            # Compute train accuracy\n",
    "            with torch.no_grad():\n",
    "                n_correct = 0\n",
    "                n_samples = 0\n",
    "                for features, labels in train_loader:\n",
    "                    features = features.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(features.float())\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    n_samples += labels.size(0)\n",
    "                    n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            train_acc = n_correct / n_samples\n",
    "\n",
    "            training_acc.append(train_acc)\n",
    "            print(f'Accuracy of the network on training set: {100*train_acc:.4f} %')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                n_correct = 0\n",
    "                n_samples = 0\n",
    "                for features, labels in val_loader:\n",
    "                    features = features.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(features.float())\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    n_samples += labels.size(0)\n",
    "                    n_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_acc = n_correct / n_samples\n",
    "            validation_acc.append(val_acc)\n",
    "            print(f'Accuracy of the network on validation set: {100*val_acc:.4} %')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j31M2YA4W-yk"
   },
   "source": [
    "Plot how the loss and the training accuracy and the validation accuracy change over the epochs. Is there a point where overfitting occurs? If you cannot spot one, answer no. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "EiAD2opxW_gY"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,3))\n",
    "ax[0].plot(range(EPOCHS), training_acc, label = \"Train Accuracy\")\n",
    "ax[0].plot(range(EPOCHS), validation_acc, label = \"Validation Accuracy\")\n",
    "ax[0].set_title('Training Accuracies')\n",
    "ax[1].plot(range(EPOCHS), losses)\n",
    "ax[1].set_title('Losses')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is point where overfitting occurs at roughly the 35th point, and as a result, the model's accuracy suffers immediately after. However, it is able to still converge back to a fairly consistent result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCRxAtjnXB55"
   },
   "source": [
    "### Part d \n",
    "Evaluate the model on the test dataset. Print out the accuracy. Does this accuracy agrees with the training accuracy showed on the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "uGAvfpWuXHLM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on test set: 81.0414452709883\n"
     ]
    }
   ],
   "source": [
    "def test(model, criterion, test_loader):\n",
    "    # Turn the model to testing mode (gradients will not be calculated)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(features.float())\n",
    "            test_loss += criterion(outputs, labels).item()  # sum up batch loss\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            \n",
    "    print(f'Accuracy of the network on test set: {100 * correct / len(test_loader.dataset)}')\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test(model, criterion, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our results from the training data, the accuarcy here lines up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnF9NTStY73w"
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "In this problem, we will investigate the effects of various common hyperparameters on the performance of a neural network. In the following cell, you can find a network class already defined for you. You can initiate network instances with different hyperparameters by changing the constructor's arguments.\n",
    "\n",
    "You are graded based on how you implement and execute the experiments. Since there is some randomness in initiating and training a neural network, there is no guarantee that you will get an expected result for an experiment or that your results should be similar to those of your peers. The expected outcome is that you execute the experiments correctly and the conclusion you get are consistent with your results. For each experiments, try to run the code multiple times and record the average results like what we did in Homework 2 (it will take some time to run, as expected when training any neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 26623,
     "status": "ok",
     "timestamp": 1636073089521,
     "user": {
      "displayName": "Kha-Dinh Luong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12651085651574501268"
     },
     "user_tz": 420
    },
    "id": "Gomk6vh0ZjQJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class mnist_network(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden_layers=1, layer_size = 100, activation=None):\n",
    "        super(mnist_network, self).__init__()\n",
    "        # layers of the network\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_size = layer_size\n",
    "        self.activation = activation\n",
    "\n",
    "        if(self.activation == 'relu'):\n",
    "            self.activation = F.relu\n",
    "        elif(self.activation == 'tanh'):\n",
    "            self.activation = torch.tanh\n",
    "\n",
    "        self.layers = nn.ModuleList([nn.Linear(784,self.layer_size)])\n",
    "        for i in range(1, self.num_hidden_layers):\n",
    "            self.layers.append(nn.Linear(self.layer_size,self.layer_size))\n",
    "        self.layers.append(nn.Linear(self.layer_size,10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # converting each image into a vector\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size,-1)\n",
    "        # rest of the forward pass \n",
    "        for i in range(self.num_hidden_layers+1):\n",
    "            x = self.layers[i](x)\n",
    "            if(self.activation is not None):\n",
    "                x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0DVPRosZkSF"
   },
   "source": [
    "Run the following code to load the MNIST dataset. For the sake of simplicity, we do not have a validation set in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eJ8_gdWvZtpU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                    transform=transform)\n",
    "\n",
    "# DataLoader is a nice tool provided by PyTorch for passing training or testing examples\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCTzQwi5GdRk"
   },
   "source": [
    "Here is an example of training and testing a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "TaSecQQAoDHb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312148\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.661013\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.153798\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.890773\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.601178\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.625142\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.615317\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.800792\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.657874\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.708585\n",
      "\n",
      "Test set: Average loss: 0.5738, Accuracy: 8494/10000 (85%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.560406\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.346600\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.263422\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.404671\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.278740\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.366671\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.221094\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.428373\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.348852\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.437842\n",
      "\n",
      "Test set: Average loss: 0.3023, Accuracy: 9119/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.201378\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.259739\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.195460\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.329895\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.216567\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.321375\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.194861\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.403567\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.313561\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.432743\n",
      "\n",
      "Test set: Average loss: 0.2783, Accuracy: 9190/10000 (92%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.164790\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.240532\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.171940\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.292491\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.187159\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.299042\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.178126\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.389411\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.284042\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.414966\n",
      "\n",
      "Test set: Average loss: 0.2626, Accuracy: 9238/10000 (92%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.143980\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.230140\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.159278\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.260374\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.167460\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.282735\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.163291\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.373959\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.258135\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.393625\n",
      "\n",
      "Test set: Average loss: 0.2491, Accuracy: 9261/10000 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "\n",
    "def train(model, criterion, train_loader, optimizer, epoch):\n",
    "    # Turn the model to training mode (gradients will be calculated)\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # We have to call zero_grad() on the optimizer to remove gradients from the previous data pass.\n",
    "        # Otherwise, the gradients will be accumulated throughout many passes.\n",
    "        optimizer.zero_grad()\n",
    "        # Pass in the data and obtain the output.\n",
    "        # When you pass the data directly by calling model(data), the model will internally pass the data through the forward() function.\n",
    "        output = model(data)\n",
    "        # Compare the output and the ground truth and calculate the loss.\n",
    "        loss = criterion(output, target)\n",
    "        # From the calculated loss, call backward() to calculate the gradients for all the paramters in the network.\n",
    "        loss.backward()\n",
    "        # Update the parameters according to the gradients. \n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, criterion, test_loader):\n",
    "    # Turn the model to testing mode (gradients will not be calculated)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 5\n",
    "# Learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# Create the model\n",
    "model = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "\n",
    "# Define the training and testing loss\n",
    "train_criterion = nn.CrossEntropyLoss()\n",
    "test_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# Define the optimizer\n",
    "# We have to specify the learning rate and the parameters that the optimizer should update during training.\n",
    "# In this case, we specify that the optimizer should update all the parameters from our model.\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Training\n",
    "    train(model, train_criterion, train_loader, optimizer, epoch)\n",
    "    # Testing\n",
    "    test(model, test_criterion, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnm5pgiLhBmM"
   },
   "source": [
    "## Part a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spGF1F_NZvt1"
   },
   "source": [
    "First, we will investigate the effect of varying the size of the hidden layer. Create 3 one-hidden-layer networks with the sizes of the hidden layers being 5, 20, 50, respectively. We will call these the 5-network, the 20-network, and the 50-network. All networks should use ReLU activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLDj0aMPaOP6"
   },
   "source": [
    "Train the 5-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "rFtFPtnOayqM",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.321440\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.208445\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.209888\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.199331\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.964409\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.998427\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.061806\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.065061\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.017438\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.168442\n",
      "\n",
      "Test set: Average loss: 1.9569, Accuracy: 22724/60000 (38%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.875798\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.841540\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.966337\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.961886\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.657763\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.735698\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.806366\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.913833\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.841156\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.993012\n",
      "\n",
      "Test set: Average loss: 1.7405, Accuracy: 25020/60000 (42%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.595093\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.622040\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.774720\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.788640\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.467511\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.523957\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.610221\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.786236\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.711848\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.908397\n",
      "\n",
      "Test set: Average loss: 1.6107, Accuracy: 26327/60000 (44%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.445289\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.504944\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.647934\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.706139\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.368004\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.414211\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.508293\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.696847\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.639622\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.865132\n",
      "\n",
      "Test set: Average loss: 1.5359, Accuracy: 27569/60000 (46%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.366419\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.438758\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.561051\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.658328\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.306920\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.341393\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.447988\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.632639\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.592777\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.835964\n",
      "\n",
      "Test set: Average loss: 1.4850, Accuracy: 28724/60000 (48%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.314261\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 1.392637\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.494554\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.626842\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.261460\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.292435\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.408106\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.585056\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.561042\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.813286\n",
      "\n",
      "Test set: Average loss: 1.4467, Accuracy: 29744/60000 (50%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.273937\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.357681\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.441472\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.602588\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.224352\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.257230\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.381082\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.548830\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.536857\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.794150\n",
      "\n",
      "Test set: Average loss: 1.4165, Accuracy: 30452/60000 (51%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.240835\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.331462\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.397474\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.583604\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.194715\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.230296\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.361830\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.519452\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.518716\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.778399\n",
      "\n",
      "Test set: Average loss: 1.3922, Accuracy: 30887/60000 (51%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.213635\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.311694\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.360272\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 1.568353\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.171842\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.209202\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.348386\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.495468\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.504374\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.764977\n",
      "\n",
      "Test set: Average loss: 1.3724, Accuracy: 31155/60000 (52%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.191072\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 1.298062\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 1.329031\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 1.555923\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 1.155095\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.191846\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 1.338663\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 1.475884\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.492763\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 1.754420\n",
      "\n",
      "Test set: Average loss: 1.3561, Accuracy: 31408/60000 (52%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Create the model\n",
    "model_5 = mnist_network(num_hidden_layers=1,layer_size=5,activation='relu')\n",
    "\n",
    "# Define the training and testing loss\n",
    "train_criterion = nn.CrossEntropyLoss()\n",
    "test_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# Define the optimizer\n",
    "# We have to specify the learning rate and the parameters that the optimizer should update during training.\n",
    "# In this case, we specify that the optimizer should update all the parameters from our model.\n",
    "optimizer = optim.SGD(model_5.parameters(),lr=lr)\n",
    "\n",
    "train_5 = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        train(model_5, train_criterion, train_loader, optimizer, epoch)\n",
    "        # Testing\n",
    "        train_5.append(test(model_5, test_criterion, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShqrwsCjazpB"
   },
   "source": [
    "Test the trained 5-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "DUbHZYLxa5yw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.3408, Accuracy: 5252/10000 (53%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52.52"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_5, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b39B1xD9bhez"
   },
   "source": [
    "Train the 20-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "yKUZhq6lbfvB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.317551\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.228011\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.240905\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.129658\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.076414\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.021779\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.940824\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.206467\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.881568\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.686215\n",
      "\n",
      "Test set: Average loss: 1.7618, Accuracy: 29224/60000 (49%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.756814\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.633259\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.617725\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.500919\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.376581\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.399405\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.417740\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.695238\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.391897\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.231258\n",
      "\n",
      "Test set: Average loss: 1.3073, Accuracy: 37845/60000 (63%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.347686\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.257826\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.144017\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.200168\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.031729\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.119624\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.194929\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.426228\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.201401\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.064206\n",
      "\n",
      "Test set: Average loss: 1.1284, Accuracy: 39874/60000 (66%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.163255\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.096178\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.941166\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.068254\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.882248\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.991922\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.078296\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.291503\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.092905\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.972041\n",
      "\n",
      "Test set: Average loss: 1.0271, Accuracy: 40855/60000 (68%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.044743\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.998121\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.823461\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.984143\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.790498\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.913644\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.003321\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.204571\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.013789\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.908981\n",
      "\n",
      "Test set: Average loss: 0.9577, Accuracy: 41634/60000 (69%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.957021\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.930360\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.745000\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.922439\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.728423\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.862371\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.946467\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.143029\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.956005\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.864212\n",
      "\n",
      "Test set: Average loss: 0.9089, Accuracy: 42818/60000 (71%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.893874\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.886713\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.689993\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.881252\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.686079\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.828426\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.907147\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.097515\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.913405\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.831053\n",
      "\n",
      "Test set: Average loss: 0.8737, Accuracy: 43894/60000 (73%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.846168\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.857395\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.650536\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.851669\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.656288\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.805344\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.879373\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.062770\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.881151\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.806592\n",
      "\n",
      "Test set: Average loss: 0.8475, Accuracy: 44695/60000 (74%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.809421\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.835507\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.620108\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.829331\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.634724\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.788642\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.857706\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.035580\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.856389\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.788387\n",
      "\n",
      "Test set: Average loss: 0.8272, Accuracy: 45331/60000 (76%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.780313\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.819248\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.596329\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.811330\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.618445\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.776778\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.839723\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 1.013773\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.837873\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.774376\n",
      "\n",
      "Test set: Average loss: 0.8110, Accuracy: 45830/60000 (76%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_20 = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "optimizer = optim.SGD(model_20.parameters(),lr=lr)\n",
    "\n",
    "train_20 = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        train(model_20, train_criterion, train_loader, optimizer, epoch)\n",
    "        # Testing\n",
    "        train_20.append(test(model_20, test_criterion, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqezVf-ybfVR"
   },
   "source": [
    "Test the trained 20-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "1JwFDxuubeki"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7815, Accuracy: 7730/10000 (77%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77.3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_20, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYFzQ_AObdp0"
   },
   "source": [
    "Train the 50-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "FpQiI7o-bzdv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298186\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.266451\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.189259\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.049706\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.027826\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.910260\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.848434\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.915681\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.741885\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.581292\n",
      "\n",
      "Test set: Average loss: 1.5920, Accuracy: 34350/60000 (57%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.628133\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.541206\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.418858\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.353191\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.233823\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.278089\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.314813\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.413110\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.310634\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.116786\n",
      "\n",
      "Test set: Average loss: 1.1994, Accuracy: 39641/60000 (66%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.258184\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.198323\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.021117\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.089432\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.936937\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.037165\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.089594\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.202116\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.127207\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.949338\n",
      "\n",
      "Test set: Average loss: 1.0377, Accuracy: 41147/60000 (69%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.078277\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.048785\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.849161\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.976227\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.805352\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.923850\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.976337\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.099885\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.025159\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.864643\n",
      "\n",
      "Test set: Average loss: 0.9517, Accuracy: 42549/60000 (71%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.973912\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.966628\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.753371\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.913916\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.730780\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.861041\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.912786\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.039184\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.958342\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.812257\n",
      "\n",
      "Test set: Average loss: 0.8994, Accuracy: 43665/60000 (73%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.906911\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.915169\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.693767\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.873584\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.682414\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.824000\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.871938\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.000326\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.912071\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.778144\n",
      "\n",
      "Test set: Average loss: 0.8642, Accuracy: 44488/60000 (74%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.860060\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.879856\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.652376\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.845697\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.649507\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.799385\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.842351\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.973335\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.877140\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.755018\n",
      "\n",
      "Test set: Average loss: 0.8387, Accuracy: 45109/60000 (75%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.824947\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.855702\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.621496\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.825064\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.626086\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.782028\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.820530\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.953454\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.848978\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.738381\n",
      "\n",
      "Test set: Average loss: 0.8190, Accuracy: 45576/60000 (76%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.797276\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.837142\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.596835\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.808428\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.608451\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.768819\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.803843\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.937671\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.825218\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.726467\n",
      "\n",
      "Test set: Average loss: 0.8032, Accuracy: 45973/60000 (77%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.775361\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.822661\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.576997\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.794504\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.594659\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.758383\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.790371\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.924863\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.805281\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.717186\n",
      "\n",
      "Test set: Average loss: 0.7902, Accuracy: 46302/60000 (77%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_50 = mnist_network(num_hidden_layers=1,layer_size=50,activation='relu')\n",
    "optimizer = optim.SGD(model_50.parameters(),lr=lr)\n",
    "\n",
    "train_50 = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        train(model_50, train_criterion, train_loader, optimizer, epoch)\n",
    "        # Testing\n",
    "        train_50.append(test(model_50, test_criterion, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnTUT4Gkb0D3"
   },
   "source": [
    "Test the trained 50-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "XcjuzPU-b7AU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.7651, Accuracy: 7840/10000 (78%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78.4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_50, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JdznrIAb7jy"
   },
   "source": [
    "Plot the training accuracies over the epochs of the networks on the same figure (there should 3 line plots/scatter plots). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "lrzcDMwhcOJP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb52c218fa0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_5, label = '5 layer')\n",
    "plt.plot(train_20, label = '20 layer')\n",
    "plt.plot(train_50, label = '50 layer')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmC8m-SRcOkP"
   },
   "source": [
    "What is your conclustion on the effect of varying the hidden layer size on the performance of a neural network trained on the MNIST dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the greater the layers size we have, the more effective the network becomes after we include more epochs, though the difference is more severe in adding the first few channels more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdlMt7WdhE4b"
   },
   "source": [
    "## Part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGBCmQHDa69o"
   },
   "source": [
    "Now, we will investigate the effect of varying the number of hidden layers. Create 3 networks with 1, 2, and 3 hidden layers, respectively. The size of all hidden layers should be 20 and the activation function is ReLU. We will call these the 1-network, the 2-network, and the 3-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "qbZfJ7Abbc2P"
   },
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Define the training and testing loss\n",
    "train_criterion = nn.CrossEntropyLoss()\n",
    "test_criterion = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9RMn8b9cm9i"
   },
   "source": [
    "Train the 1-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, \n",
    "\n",
    "---\n",
    "\n",
    "record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "km6IknYlcm9j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.351165\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.237006\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.093750\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.929818\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.974377\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.875087\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.556570\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.673218\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.564982\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.275017\n",
      "\n",
      "Test set: Average loss: 1.4415, Accuracy: 36236/60000 (60%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.484977\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.304284\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.314311\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.219861\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.491711\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.412746\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.019302\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.235540\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.194130\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.936995\n",
      "\n",
      "Test set: Average loss: 1.1157, Accuracy: 40358/60000 (67%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.199095\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.981038\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.004695\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.984115\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.298311\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.197006\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.807701\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.035766\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.028638\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.810643\n",
      "\n",
      "Test set: Average loss: 0.9677, Accuracy: 41877/60000 (70%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.072543\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.834296\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.839809\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.861094\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.042221\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.997864\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.596406\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.840835\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.862188\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.621421\n",
      "\n",
      "Test set: Average loss: 0.7775, Accuracy: 47279/60000 (79%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.874938\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.700077\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.613795\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.705972\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.930886\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.914301\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.497558\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.738562\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.768493\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.564168\n",
      "\n",
      "Test set: Average loss: 0.7090, Accuracy: 47976/60000 (80%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.796533\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.618517\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.556085\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.639029\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.880295\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.858960\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.450602\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.685937\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.709698\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.533448\n",
      "\n",
      "Test set: Average loss: 0.6645, Accuracy: 48341/60000 (81%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.745914\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.567364\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.519716\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.597483\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.838515\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.818532\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.419259\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.652005\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.670342\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.512509\n",
      "\n",
      "Test set: Average loss: 0.6324, Accuracy: 48574/60000 (81%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.708307\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.532710\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.493294\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.571182\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.806093\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.789693\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.395340\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.627926\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.641709\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.496980\n",
      "\n",
      "Test set: Average loss: 0.6083, Accuracy: 48752/60000 (81%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.679113\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.508260\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.473366\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.553933\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.780387\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.768840\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.376863\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.610905\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.619284\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.485507\n",
      "\n",
      "Test set: Average loss: 0.5895, Accuracy: 48896/60000 (81%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.654934\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.490679\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.458022\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.541221\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.760644\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.752690\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.362441\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.598285\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.601211\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.475790\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_1 = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "optimizer = optim.SGD(model_1.parameters(),lr=lr)\n",
    "\n",
    "train_1 = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        train(model_1, train_criterion, train_loader, optimizer, epoch)\n",
    "        train_1.append(test(model_1, test_criterion, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHW3-J4vcm9k"
   },
   "source": [
    "Test the trained 1-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "iROIK0lgcm9m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5615, Accuracy: 8222/10000 (82%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82.22"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_1, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37h7g55ecm9n"
   },
   "source": [
    "Train the 2-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "_c7TL_5Icm9o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.325221\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.305492\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.301848\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.260953\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.275739\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.241323\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.214609\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.261936\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.209897\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.157484\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.153461\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.113545\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.206012\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.969792\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.003573\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.958805\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.955050\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.122275\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.954876\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.888553\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.820627\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.759484\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.991349\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.668006\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.688461\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.643827\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.612760\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.944686\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.683515\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.552363\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.531962\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.490770\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.727600\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.419731\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.418502\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.418193\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.333709\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.755605\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.451314\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.286769\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.314415\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.280101\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.512478\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.223100\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.227224\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.251781\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.156390\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.611154\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.304353\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.128117\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.171854\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 1.143544\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.372679\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.099954\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.122496\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.152708\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.057380\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.519413\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.212132\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.041719\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.075606\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.063624\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.289917\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.026551\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.052520\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.094956\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.000678\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.456095\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.143332\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.989034\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.006169\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.014295\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.236329\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.978822\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.003454\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.061629\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.965616\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.410911\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.090428\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.952378\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.950642\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.979889\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.197840\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.949107\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.966146\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.038035\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.942617\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.378685\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.048139\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.921625\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.906483\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.953474\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 1.168054\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.928759\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.937555\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.021101\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.924393\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 1.354418\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.015701\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.897101\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2 = mnist_network(num_hidden_layers=2,layer_size=20,activation='relu')\n",
    "optimizer = optim.SGD(model_2.parameters(),lr=lr)\n",
    "\n",
    "train_2 = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        train(model_2, train_criterion, train_loader, optimizer, epoch)\n",
    "        train_2.append(test(model_1, test_criterion, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fJGIL1Hcm9p"
   },
   "source": [
    "Test the trained 2-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "m7Sr0On6cm9p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.0217, Accuracy: 6507/10000 (65%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65.07"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_2, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3KteClMcm9q"
   },
   "source": [
    "Train the 3-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "thWHY7rWcm9r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302562\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.303644\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.302970\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.314267\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.308614\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.293324\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.290712\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.297036\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.299915\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.299205\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.294710\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.294097\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.296114\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.309779\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.296661\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.280991\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.271788\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.274253\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.280630\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.274354\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.258916\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.260136\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.279160\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.270591\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.271703\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.248841\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.221462\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.222509\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.233599\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.223000\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.188464\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.190552\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.249191\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.188062\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.227818\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.188554\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.112998\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.122437\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.144279\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.126651\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.067403\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.069472\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.208099\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.095124\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.169632\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.114424\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.997360\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.018411\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.065549\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.056535\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.979900\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 1.965487\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 2.150515\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 2.040139\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.105255\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 2.031651\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.899890\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.911468\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.982568\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.994272\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.886821\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.843724\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 2.060527\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.976303\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 2.010366\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.918582\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.754093\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.763331\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.848082\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.886458\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.722288\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.634319\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.916767\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.803691\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.836590\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.719116\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.522881\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.605103\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.621219\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.699002\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.511993\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.376002\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.673275\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 1.577297\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.609170\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.463794\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.249712\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.425193\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.391497\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.343554\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.301243\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 1.156901\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 1.378169\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 1.381629\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 1.437446\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.292783\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 1.017623\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 1.284842\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.243222\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 1.135275\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 49030/60000 (82%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_3 = mnist_network(num_hidden_layers=3,layer_size=20,activation='relu')\n",
    "optimizer = optim.SGD(model_3.parameters(),lr=lr)\n",
    "\n",
    "train_3 = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        train(model_3, train_criterion, train_loader, optimizer, epoch)\n",
    "        train_3.append(test(model_1, test_criterion, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cB4nTkYecm9s"
   },
   "source": [
    "Test the trained 3-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "176L8ou0cm9t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.2027, Accuracy: 6328/10000 (63%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63.28"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_3, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CW0D0CWWcm9t"
   },
   "source": [
    "Plot the training accuracies over the epochs of the networks on the same figure (there should 3 line plots/scatter plots). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "gW6JCxpmcm9t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb53012fee0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_1, label = '1 layer')\n",
    "plt.plot(train_2, label = '2 layer')\n",
    "plt.plot(train_3, label = '3 layer')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65eqaTMbcm9u"
   },
   "source": [
    "What is your conclustion on the effect of varying the number of hidden layers on the performance of a neural network trained on the MNIST dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My training accuracies for the 2nd and 3rd layer were approximately the same, so adding another hidden layer will train the model faster, but doesn't necessarily make the model more accurate after a certain number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfrIobY5hJe5"
   },
   "source": [
    "## Part c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX_rfiw5c6ZL"
   },
   "source": [
    "Next, we will investigate the effects of varying the activation functions on a neural network. Create 3 networks. The first network has Sigmoid activation (Sigmoid-network). The second network has ReLU activation (ReLU-network). The third network has Tanh activation (Tanh-network). All networks have one hidden layer with size 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "vboiquSGc4cE"
   },
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Define the training and testing loss\n",
    "train_criterion = nn.CrossEntropyLoss()\n",
    "test_criterion = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py5w8S3Cd9E0"
   },
   "source": [
    "Train the Sigmoid-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "3dGgpGwyd9E2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301595\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.309707\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.303554\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.304775\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.297797\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.294732\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.285025\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.315665\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.292294\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.287698\n",
      "\n",
      "Test set: Average loss: 2.2899, Accuracy: 5922/60000 (10%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.287932\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.295752\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.292507\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.288934\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.284253\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.282168\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.270616\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.304687\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.279253\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.274067\n",
      "\n",
      "Test set: Average loss: 2.2768, Accuracy: 8358/60000 (14%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.274616\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.281997\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.281560\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.273484\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.271089\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.269976\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.256835\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.294069\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.266760\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.261118\n",
      "\n",
      "Test set: Average loss: 2.2642, Accuracy: 11930/60000 (20%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.262172\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.268850\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.270884\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.259145\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.258778\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.258431\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.243957\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.284070\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.255064\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.249102\n",
      "\n",
      "Test set: Average loss: 2.2525, Accuracy: 14705/60000 (25%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.250757\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.256540\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.260612\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.246204\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.247457\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.247637\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.232019\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.274724\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.244221\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.238004\n",
      "\n",
      "Test set: Average loss: 2.2417, Accuracy: 18829/60000 (31%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.240258\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 2.245085\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 2.250801\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 2.234568\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.237023\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 2.237560\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.220931\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 2.265945\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.234161\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 2.227687\n",
      "\n",
      "Test set: Average loss: 2.2316, Accuracy: 23221/60000 (39%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.230493\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 2.234391\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 2.241434\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 2.223974\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 2.227299\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 2.228103\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 2.210568\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 2.257609\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 2.224752\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 2.217993\n",
      "\n",
      "Test set: Average loss: 2.2221, Accuracy: 26780/60000 (45%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.221304\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 2.224339\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 2.232463\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 2.214160\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 2.218123\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 2.219155\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 2.200808\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 2.249600\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 2.215869\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 2.208789\n",
      "\n",
      "Test set: Average loss: 2.2131, Accuracy: 29391/60000 (49%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.212569\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 2.214814\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 2.223830\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 2.204919\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 2.209368\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 2.210621\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 2.191550\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 2.241825\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 2.207404\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 2.199971\n",
      "\n",
      "Test set: Average loss: 2.2045, Accuracy: 31297/60000 (52%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.204198\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 2.205724\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 2.215479\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 2.196106\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 2.200945\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 2.202423\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 2.182712\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 2.234219\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 2.199275\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 2.191464\n",
      "\n",
      "Test set: Average loss: 2.1961, Accuracy: 32870/60000 (55%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_s = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "model_s.activation = torch.nn.Sigmoid()\n",
    "optimizer = optim.SGD(model_s.parameters(),lr=lr)\n",
    "\n",
    "train_s = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        train(model_s, train_criterion, train_loader, optimizer, epoch)\n",
    "        train_s.append(test(model_s, test_criterion, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6PTSxcDd9E2"
   },
   "source": [
    "Test the trained Sigmoid-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "qKhk1nq7d9E3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.1945, Accuracy: 5561/10000 (56%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55.61"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_s, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hpbsbN1d9E4"
   },
   "source": [
    "Train the ReLU-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "fu3wbDf2d9E7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.281887\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.217042\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.294579\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.148053\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.133491\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.118838\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.027254\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.099699\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.917426\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.996129\n",
      "\n",
      "Test set: Average loss: 1.8640, Accuracy: 24642/60000 (41%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.756914\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.699863\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.960794\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.649650\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.533370\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.644837\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.634581\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.531609\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.450205\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.526274\n",
      "\n",
      "Test set: Average loss: 1.3516, Accuracy: 37826/60000 (63%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.316581\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.239222\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.241424\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.120924\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.940467\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.094005\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.064625\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.016214\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.966735\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.014855\n",
      "\n",
      "Test set: Average loss: 0.9268, Accuracy: 45260/60000 (75%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.936652\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.924614\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.890812\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.777797\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.692548\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.866574\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.875229\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.816575\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.802185\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.883845\n",
      "\n",
      "Test set: Average loss: 0.7927, Accuracy: 46121/60000 (77%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.797789\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.807727\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.766917\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.682856\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.597885\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.769941\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.796269\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.735836\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.723555\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.824592\n",
      "\n",
      "Test set: Average loss: 0.7276, Accuracy: 46612/60000 (78%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.723001\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.749109\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.704244\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.633270\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.544363\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.718154\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.751379\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.692432\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.676200\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.791701\n",
      "\n",
      "Test set: Average loss: 0.6883, Accuracy: 46962/60000 (78%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.674238\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.713985\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.664369\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.601350\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.508732\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.685534\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.721416\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.665319\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.645631\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.771619\n",
      "\n",
      "Test set: Average loss: 0.6615, Accuracy: 47194/60000 (79%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.638965\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.690548\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.636072\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.578550\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.482469\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.662586\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.700130\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.646281\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.623952\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.758897\n",
      "\n",
      "Test set: Average loss: 0.6418, Accuracy: 47407/60000 (79%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.612075\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.674097\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.614408\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.561181\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.462030\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.646933\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.683693\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.632435\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.607484\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.749298\n",
      "\n",
      "Test set: Average loss: 0.6264, Accuracy: 47583/60000 (79%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.590189\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.662012\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.597644\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.547133\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.445923\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.633784\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.670642\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.622273\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.593935\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.742145\n",
      "\n",
      "Test set: Average loss: 0.6140, Accuracy: 47711/60000 (80%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_r = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "optimizer = optim.SGD(model_r.parameters(),lr=lr)\n",
    "\n",
    "train_r = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        train(model_r, train_criterion, train_loader, optimizer, epoch)\n",
    "        train_r.append(test(model_r, test_criterion, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYOANVPWd9E7"
   },
   "source": [
    "Test the trained ReLU-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Cw_lcZAsd9E8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.6020, Accuracy: 7980/10000 (80%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79.8"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_r, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66X8ymnLd9E8"
   },
   "source": [
    "Train the Tanh-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "m8HulP31d9E9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.339976\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.083477\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.929702\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.899072\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.778455\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.760510\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.704436\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.745781\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.733356\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.657083\n",
      "\n",
      "Test set: Average loss: 1.6586, Accuracy: 43411/60000 (72%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.653630\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.630653\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.619556\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.623303\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.578895\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.604777\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.546601\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.602877\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.608471\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.542407\n",
      "\n",
      "Test set: Average loss: 1.5460, Accuracy: 47812/60000 (80%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.541033\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.518264\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.525522\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.526270\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.500430\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.529891\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.465365\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.530134\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.534062\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.474018\n",
      "\n",
      "Test set: Average loss: 1.4782, Accuracy: 49686/60000 (83%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.471897\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.448209\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.465878\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.464414\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.446960\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.474476\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.407470\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.478619\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.479325\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.422948\n",
      "\n",
      "Test set: Average loss: 1.4276, Accuracy: 50649/60000 (84%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.419244\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.396086\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.419674\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.418471\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.403967\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.428925\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.361804\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.437657\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.435258\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.381601\n",
      "\n",
      "Test set: Average loss: 1.3863, Accuracy: 51214/60000 (85%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.375681\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 1.353973\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.380950\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.381694\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.367401\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.390481\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.324276\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.403409\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.397951\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.346887\n",
      "\n",
      "Test set: Average loss: 1.3515, Accuracy: 51648/60000 (86%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.338529\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.318748\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.347319\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.351182\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.335975\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.358059\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.292866\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.374063\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.365597\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.317237\n",
      "\n",
      "Test set: Average loss: 1.3215, Accuracy: 51971/60000 (87%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.306477\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.288790\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.317505\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.325275\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.308811\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.330597\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.266326\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.348563\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.337181\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.291643\n",
      "\n",
      "Test set: Average loss: 1.2954, Accuracy: 52252/60000 (87%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.278593\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.263040\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.290709\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 1.302788\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.285059\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.307066\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.243746\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.326226\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.312101\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.269385\n",
      "\n",
      "Test set: Average loss: 1.2724, Accuracy: 52532/60000 (88%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.254116\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 1.240688\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 1.266507\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 1.282884\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 1.264085\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.286670\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 1.224357\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 1.306602\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.289986\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 1.249912\n",
      "\n",
      "Test set: Average loss: 1.2522, Accuracy: 52733/60000 (88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_t = mnist_network(num_hidden_layers=1,layer_size=20,activation='tanh')\n",
    "optimizer = optim.SGD(model_t.parameters(),lr=lr)\n",
    "\n",
    "train_t = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        train(model_t, train_criterion, train_loader, optimizer, epoch)\n",
    "        train_t.append(test(model_t, test_criterion, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYt6o7Aed9E-"
   },
   "source": [
    "Test the trained Tanh-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "q7-YK5gVd9E_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.2454, Accuracy: 8842/10000 (88%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88.42"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_t, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIGx8kPTd9FA"
   },
   "source": [
    "Plot the training accuracies over the epochs of the networks on the same figure (there should 3 line plots/scatter plots). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "MVy4iVnMd9FA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb52f05eeb0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_s, label = 'Sigmoid')\n",
    "plt.plot(train_r, label = 'Relu')\n",
    "plt.plot(train_t, label = 'Tanh')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCGjP911d9FB"
   },
   "source": [
    "What is your conclustion on the effect of varying the activation functions on the performance of a neural network trained on MNIST dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tanh function appeared to give the highest accuracy, while the relu converged to a lower point and the sigmoid did not seem converge after 10 epochs, so perhaps the sigmoid still has potential as a slow learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiAZ36IdhMvG"
   },
   "source": [
    "## Part d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8H1N3i-derH7"
   },
   "source": [
    "Finally, we will look into the effect of varying the value of the learning rate on the performance of a neural network. Create a network with one hidden layer of size 20 and ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "WKp-TYHlea9w"
   },
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# Define the training and testing loss\n",
    "train_criterion = nn.CrossEntropyLoss()\n",
    "test_criterion = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUFgUcGUfG7z"
   },
   "source": [
    "Train the network on the MNIST dataset for 10 epochs. Set the learning rate to be 0.1. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "KtLuEUNKfG72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.324512\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.312341\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.254524\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.313385\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.211388\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.328997\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.222577\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.268873\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.236029\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.303315\n",
      "\n",
      "Test set: Average loss: 0.2253, Accuracy: 55840/60000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.139828\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.197228\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.132884\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.228968\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.161075\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.236868\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.160275\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.241555\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.163595\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.269563\n",
      "\n",
      "Test set: Average loss: 0.1685, Accuracy: 56862/60000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.112471\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.124982\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.110417\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.224524\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.150086\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.196960\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.100432\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.195327\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.163248\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.251062\n",
      "\n",
      "Test set: Average loss: 0.1504, Accuracy: 57184/60000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.111182\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.092998\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.098265\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.173106\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.126528\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.194421\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.112932\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.166068\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.156627\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.228048\n",
      "\n",
      "Test set: Average loss: 0.1362, Accuracy: 57454/60000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.104668\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.077030\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.079599\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.141657\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.112371\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.201750\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.135284\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.148029\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.161087\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.209256\n",
      "\n",
      "Test set: Average loss: 0.1322, Accuracy: 57533/60000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.115813\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.071467\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.068759\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.116211\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.094429\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.208769\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.145213\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.127880\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.151395\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.181021\n",
      "\n",
      "Test set: Average loss: 0.1244, Accuracy: 57652/60000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.112015\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.057496\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.071202\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.098883\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.096744\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.195164\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.140555\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.116852\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.144675\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.171584\n",
      "\n",
      "Test set: Average loss: 0.1157, Accuracy: 57846/60000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.100874\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.049888\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.065431\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.098656\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.091312\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.178458\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.141795\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.101325\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.149491\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.160454\n",
      "\n",
      "Test set: Average loss: 0.1144, Accuracy: 57871/60000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.097544\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.042563\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.057320\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.081184\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.093119\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.171133\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.133179\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.097496\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.144246\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.158419\n",
      "\n",
      "Test set: Average loss: 0.1114, Accuracy: 57920/60000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.088668\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.036727\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.055895\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.075764\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.089220\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.145999\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.131782\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.099017\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.146621\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.144274\n",
      "\n",
      "Test set: Average loss: 0.1098, Accuracy: 57939/60000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_a = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "lr = 0.1\n",
    "optimizer = optim.SGD(model_a.parameters(),lr=lr)\n",
    "\n",
    "train_a = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        train(model_a, train_criterion, train_loader, optimizer, epoch)\n",
    "        train_a.append(test(model_a, test_criterion, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oC5CD3zmfG73"
   },
   "source": [
    "Test the trained network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "yT04G-BLfG74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1575, Accuracy: 9559/10000 (96%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95.59"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_a, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REVtrsiqfG75"
   },
   "source": [
    "Train the network on the MNIST dataset for 10 epochs. Set the learning rate to be 0.01. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "aEXtS_XEfG76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298980\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.415908\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.323044\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.202785\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.837407\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.798797\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.765324\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.963126\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.911553\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.979912\n",
      "\n",
      "Test set: Average loss: 0.8162, Accuracy: 43478/60000 (72%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.675400\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.627203\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.695437\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.908638\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.662911\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.675273\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.636540\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.893298\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.800239\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.931625\n",
      "\n",
      "Test set: Average loss: 0.7446, Accuracy: 44339/60000 (74%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.583531\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.592212\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.620640\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.878048\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.623821\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.640307\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.600952\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.887341\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.756342\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.910970\n",
      "\n",
      "Test set: Average loss: 0.7137, Accuracy: 44703/60000 (75%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.550697\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.568239\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.588444\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.865209\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.600921\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.621725\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.584999\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.888465\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.731478\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.893326\n",
      "\n",
      "Test set: Average loss: 0.6933, Accuracy: 44926/60000 (75%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.536483\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.543123\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.567843\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.850565\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.588987\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.613429\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.573629\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.886284\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.710770\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.879190\n",
      "\n",
      "Test set: Average loss: 0.6778, Accuracy: 45138/60000 (75%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.528496\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.522225\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.554385\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.831553\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.574143\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.607277\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.562679\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.885489\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.696746\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.866379\n",
      "\n",
      "Test set: Average loss: 0.6653, Accuracy: 45300/60000 (76%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.517867\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.509298\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.544725\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.813968\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.563698\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.588916\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.551228\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.879637\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.675821\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.856169\n",
      "\n",
      "Test set: Average loss: 0.6543, Accuracy: 45436/60000 (76%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.509052\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.495396\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.537554\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.799301\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.552130\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.579908\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.541948\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.874071\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.661048\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.845504\n",
      "\n",
      "Test set: Average loss: 0.6451, Accuracy: 45549/60000 (76%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.497810\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.481755\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.533681\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.784368\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.544574\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.570323\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.535745\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.865992\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.650129\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.834754\n",
      "\n",
      "Test set: Average loss: 0.6371, Accuracy: 45663/60000 (76%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.486285\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.469220\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.528691\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.770642\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.537360\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.562156\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.531304\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.856225\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.638969\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.825931\n",
      "\n",
      "Test set: Average loss: 0.6304, Accuracy: 45740/60000 (76%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_b = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "lr = 0.01\n",
    "optimizer = optim.SGD(model_b.parameters(),lr=lr)\n",
    "\n",
    "train_b = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        train(model_b, train_criterion, train_loader, optimizer, epoch)\n",
    "        train_b.append(test(model_b, test_criterion, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2dLcxgdfG77"
   },
   "source": [
    "Test the trained network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "pT9_RHk_fG78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.6322, Accuracy: 7638/10000 (76%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.38"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_b, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJu3jwdKfG79"
   },
   "source": [
    "Train the network on the MNIST dataset for 10 epochs. Set the learning rate to 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "U_GDwCxkfG7-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.353023\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.293145\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.178953\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.157175\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.056528\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.012466\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.778736\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.817686\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.699716\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.477475\n",
      "\n",
      "Test set: Average loss: 1.5282, Accuracy: 39361/60000 (66%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.623578\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.456735\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.307255\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.257192\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.343965\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.304283\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.936449\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.201425\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.149564\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.922443\n",
      "\n",
      "Test set: Average loss: 1.0156, Accuracy: 45395/60000 (76%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.113865\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.934241\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.871322\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.878541\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.080655\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.046979\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.659038\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.930863\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.927686\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.744721\n",
      "\n",
      "Test set: Average loss: 0.8352, Accuracy: 46789/60000 (78%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.937301\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.753811\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.700586\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.741067\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.973972\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.922415\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.550535\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.804712\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.815086\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.666530\n",
      "\n",
      "Test set: Average loss: 0.7466, Accuracy: 47537/60000 (79%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.844682\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.662456\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.614482\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.673963\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.908774\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.849929\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.489985\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.732203\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.745532\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.622307\n",
      "\n",
      "Test set: Average loss: 0.6925, Accuracy: 47901/60000 (80%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.782359\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.606320\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.562650\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.632474\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.862520\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.803974\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.450305\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.685472\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.698506\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.593154\n",
      "\n",
      "Test set: Average loss: 0.6556, Accuracy: 48193/60000 (80%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.737935\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.569005\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.527740\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.604454\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.828974\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.773334\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.421254\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.652739\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.664618\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.572006\n",
      "\n",
      "Test set: Average loss: 0.6288, Accuracy: 48440/60000 (81%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.704114\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.543282\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.501873\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.584986\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.802990\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.751637\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.399112\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.628883\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.639128\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.556578\n",
      "\n",
      "Test set: Average loss: 0.6085, Accuracy: 48618/60000 (81%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.677444\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.524357\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.481715\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.570264\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.782828\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.735200\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.381736\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.611363\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.618929\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.544350\n",
      "\n",
      "Test set: Average loss: 0.5924, Accuracy: 48791/60000 (81%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.655728\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.510754\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.465433\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.558340\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.766528\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.722264\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.367999\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.597598\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.602003\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.534860\n",
      "\n",
      "Test set: Average loss: 0.5793, Accuracy: 48920/60000 (82%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_c = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "lr = 0.001\n",
    "optimizer = optim.SGD(model_c.parameters(),lr=lr)\n",
    "\n",
    "train_c = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        train(model_c, train_criterion, train_loader, optimizer, epoch)\n",
    "        train_c.append(test(model_c, test_criterion, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HbaGOWRfG7_"
   },
   "source": [
    "Test the trained network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "hp6jgPocfG8A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5652, Accuracy: 8212/10000 (82%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82.12"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model_c, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCAv1ujIfG8A"
   },
   "source": [
    "Plot the training accuracies over the epochs of the scenarios on the same figure (there should 3 line plots/scatter plots). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "iTazEXPkfG8B"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb55641b940>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_a, label = '0.1')\n",
    "plt.plot(train_b, label = '0.01')\n",
    "plt.plot(train_c, label = '0.001')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLal-1hxfG8B"
   },
   "source": [
    "What is your conclustion on the effect of varying the learning rate on the performance of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate of 0.1 produced the best results, and the general trend that I saw in these three learning rates was that the smaller the learning rate, the longer it took the network to converge to a value of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9FJtJgufv-n"
   },
   "source": [
    "## REMARK for Problem 2\n",
    "\n",
    "You have observed the effects of varying different hyperparameters on the performance of a neural network **on the MNIST dataset**. However, keep in mind that these trends only apply for **the MNIST dataset** and should not be carried to another problem. There is no single hyperparameter settings that works for all problems. As you do more problems, you will build up your intuitions about the hyperparameters so that you can quickly deploy a good model. For example, people observed that setting the learning rate = 0.001 often works the best, though it is not always the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHSaycfVdHRJ"
   },
   "source": [
    "## Problem 3\n",
    "\n",
    "Experimenting with **k-anonimity, i-diversity, and t-closeness**. \n",
    "\n",
    "Consider a dataset, for example, with 3 ordinary attributes and 1 sensitive attribute. Let the 3 ordinary attributes be Age, Sex, and Education and the sensitive attribute be Income, each row in this dataset is of the form:\n",
    "\n",
    "$$\n",
    "    [Age, Sex, Education, Income]\n",
    "$$\n",
    "\n",
    "A hacker is interested in knowing the sensitive attribute Income. When the dataset is designed so that if complies with either **k-anomity**, **i-diversity**, and/or **t-closeness**, even if he or she somehow figures out the values of the three, the hacker may not retrive the sensitive information accurately. In general, **k-anomity** is weaker than **i-diversity**, which, in turn, is weaker than **t-closeness**.\n",
    "\n",
    "By definition, **k-anomity** means that there is at least **k** different rows in the table of which ordinary values are a particular combination of Age, Sex, and Education. For example, the hacker knows the information of the person of interest is Age = 31, Sex = Female, and Education = BS. He or she looks into the data table and found that there are 3 rows with that combination:\n",
    "\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=300k]\n",
    "$$\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=70k]\n",
    "$$\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=20k]\n",
    "$$\n",
    "\n",
    "The hacker cannot tell accurately what the income of the person is because it can be one of the 3 values shown. This particular combination of information has 3-anomity. If every combination corresponds to at least 3 rows, then the dataset has 3-anomity.\n",
    "\n",
    "a) Let's look at the dataset **\"table.csv\"**. Let the sensitive attribute be **education** and others be ordinary attributes. Calculate the anomity of the dataset (the value **k**). First, find all the posible combinations of the ordinary attributes that exists in the dataset. After that, determine the anomity for each combination. The anomity of the dataset is the smallest anomity among the combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "Qrm7S5y5dHRJ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>11th</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  age  education   race     sex\n",
       "0           0   39  Bachelors  White    Male\n",
       "1           1   50  Bachelors  White    Male\n",
       "2           2   38    HS-grad  White    Male\n",
       "3           3   53       11th  Black    Male\n",
       "4           4   28  Bachelors  Black  Female"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "table = pd.read_csv('table.csv')\n",
    "table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "AGqI5mltdHRK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonimity is 1\n"
     ]
    }
   ],
   "source": [
    "anonimity = min(table.groupby(['age', 'race', 'sex']).count()['education'])\n",
    "print(f'Anonimity is {anonimity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixGLowBjdHRK"
   },
   "source": [
    "We can improve the **k-anomity** of the dataset by \"suppressing\" the ordinary attributes. Suppressing means reducing the resolution of the attribute's value. For this problem, let's suppress Age by replacing the exact age with an age range. For example, instead of leaving age = 32, replace it with age = 30-40. Apply this to **\"table.csv\"** with the ranges {<20, 20-30, 30-50, >50}. Check if the anomity improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ageRange(age):\n",
    "    if age < 20:\n",
    "        return '<20'\n",
    "    elif age <= 30:\n",
    "        return '20-30'\n",
    "    elif age <=50:\n",
    "        return '30-50'\n",
    "    else:\n",
    "        return '>50'\n",
    "\n",
    "table['age_range'] = table['age'].apply(ageRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonimity is 4\n"
     ]
    }
   ],
   "source": [
    "anonimity = min(table.groupby(['age_range', 'race', 'sex']).count()['education'])\n",
    "print(f'Anonimity is {anonimity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anonimity improves by 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv-RmXqCdHRK"
   },
   "source": [
    "**K-anomity** is nice, however, it fails in many cases. If the rows which share a combination of ordinary attributes have only a few values for the sensitive attribute, then it is not much better than having no anomity at all. For example, consider:\n",
    "\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=300k]\n",
    "$$\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=20k]\n",
    "$$\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=20k]\n",
    "$$\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=20k]\n",
    "$$\n",
    "\n",
    "When **k-anomity** fails in the second case, **i-diversity** comes to the rescue. **I-diversity** states that the rows of a particular combination of information must have at least i different values for the sensitive attribute. The above example has 2-diversity, which is not good. \n",
    "\n",
    "b) Calculate the **i-diversity** of the dataset **\"table.csv\"**. Follow similar steps as in part a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "eZYCvcukdHRK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>11th</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  age  education   race     sex\n",
       "0           0   39  Bachelors  White    Male\n",
       "1           1   50  Bachelors  White    Male\n",
       "2           2   38    HS-grad  White    Male\n",
       "3           3   53       11th  Black    Male\n",
       "4           4   28  Bachelors  Black  Female"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.read_csv('table.csv')\n",
    "table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>education</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <th>Amer-Indian-Eskimo</th>\n",
       "      <th>Female</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">67</th>\n",
       "      <th>Asian-Pac-Islander</th>\n",
       "      <th>Male</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amer-Indian-Eskimo</th>\n",
       "      <th>Male</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <th>Amer-Indian-Eskimo</th>\n",
       "      <th>Male</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <th>Other</th>\n",
       "      <th>Female</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               education\n",
       "age race               sex              \n",
       "41  Amer-Indian-Eskimo Female          1\n",
       "67  Asian-Pac-Islander Male            1\n",
       "    Amer-Indian-Eskimo Male            1\n",
       "39  Amer-Indian-Eskimo Male            1\n",
       "66  Other              Female          1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.groupby([\"age\",\"race\", \"sex\"]).agg({\"education\":\"nunique\"}).sort_values('education').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The i-diversity is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qWt9JvTdHRK"
   },
   "source": [
    "Suppressing an attribute can also improve the **i-diversity** of the dataset. Repeat the suppression as in **part a** and check if the diversity improves. If it does not, consider further suppress age by using the range {<20, 20-50, >50}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>education</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_range</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;20</th>\n",
       "      <th>Amer-Indian-Eskimo</th>\n",
       "      <th>Male</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;50</th>\n",
       "      <th>Other</th>\n",
       "      <th>Female</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">&lt;20</th>\n",
       "      <th>Amer-Indian-Eskimo</th>\n",
       "      <th>Female</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian-Pac-Islander</th>\n",
       "      <th>Male</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <th>Male</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     education\n",
       "age_range race               sex              \n",
       "<20       Amer-Indian-Eskimo Male            3\n",
       ">50       Other              Female          4\n",
       "<20       Amer-Indian-Eskimo Female          4\n",
       "          Asian-Pac-Islander Male            4\n",
       "          Other              Male            5"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ageRange_i (age):\n",
    "    if age < 20:\n",
    "        return '<20'\n",
    "    elif age <=50:\n",
    "        return '20-50'\n",
    "    else:\n",
    "        return '>50'\n",
    "\n",
    "table['age_range'] = table['age'].apply(ageRange_i)\n",
    "table.groupby([\"age_range\",\"race\", \"sex\"]).agg({\"education\":\"nunique\"}).sort_values('education').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The i-diversity is improved from 1 to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX1lsm8ydHRK"
   },
   "source": [
    "**T-closeness** is even better than **i-diversity**. **T-closeness** requires that for every combination of information, the distribution of the sensitive attribute's value among the corresponding rows must be close to the overall distribution of the sensitive attribute's value for the whole dataset. Distance between distribution is calculated using the Earth Mover Distance (EMD). The dataset has **t-closeness** if no distance exceeds **t**. \n",
    "\n",
    "c) Calculate the overall distribution of **education**. Find the **t-closeness** of the dataset (largest distance between any combination's distribution of marital-status and the overall distribution).\n",
    "\n",
    "You can use **scipy.stats.wasserstein_distance** to calculate the EMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "BjR4iG2bdHRK"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "table = pd.read_csv('table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['race'] = pd.factorize(table['race'])[0]\n",
    "table['sex'] = pd.factorize(table['sex'])[0]\n",
    "table['education'] = pd.factorize(table['education'])[0]\n",
    "\n",
    "ages = np.unique(table['age'])\n",
    "races = np.unique(table['race'])\n",
    "sexes = np.unique(table['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "education = (table['education'])\n",
    "distances = []\n",
    "\n",
    "for age in range(len(ages)):\n",
    "    for race in range(len(races)):\n",
    "        for sex in range(len(sexes)):\n",
    "            temp = table[(table['age']==age) & (table['race']==race) & (table['sex']==sex)]\n",
    "            if temp.empty:\n",
    "                distances.append(wasserstein_distance([0], education)) \n",
    "            else:\n",
    "                use = temp.iloc[:, 2]\n",
    "                distances.append(wasserstein_distance(use, education))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.44312068583628"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closeness = max(distances) - min(distances)\n",
    "closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "p9FJtJgufv-n"
   ],
   "name": "HW3_F21.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
